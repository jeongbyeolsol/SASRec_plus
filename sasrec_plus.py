# -*- coding: utf-8 -*-
"""SASRec_plus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C1brQf4D_XFZQCQU2b59cuAR0kTzCHoA
"""

import json
import os
import tqdm
from tqdm import tqdm
from datetime import datetime
import random
import numpy as np
import math
import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch import optim
import torch.nn.functional as F

"""*가장* 작은 item index: 1
유저 수 = 100000
아이템 수 = 19738
embedding용 num_items = 19738
"""

def set_seed(seed):
    random.seed(seed)                     # Python random 고정
    np.random.seed(seed)                  # NumPy 난수 고정
    torch.manual_seed(seed)               # CPU 난수 고정

    # GPU가 있을 경우
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)      # GPU 난수 고정
        torch.cuda.manual_seed_all(seed)  # multi-GPU 난수 고정

    # 연산 결정적(deterministic) 모드
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

import torch

def hr_ndcg_at_k(scores: torch.Tensor,
                 pos_indices: torch.Tensor,
                 k: int = 10):
    """
    scores: (B, C)    - 각 유저별 후보 아이템 점수
    pos_indices: (B,) - 각 유저에서 '정답 아이템'이 후보 리스트 내에서 가진 index (0~C-1)
    k: top-k (보통 5, 10 등)

    return: hr@k, ndcg@k (스칼라 float)
    """
    device = scores.device
    B, C = scores.shape

    # 1) 점수 기준으로 내림차순 정렬 → 랭킹 인덱스 얻기
    # ranking[b] = 해당 유저에서, 점수가 큰 순서대로 후보 인덱스 나열
    _, ranking = scores.sort(dim=1, descending=True)  # (B, C)

    # 2) 각 유저별로 '정답 인덱스'가 랭킹에서 어디 위치에 있는지 찾기
    # ranking: (B, C) 안에서 값이 pos_indices[b]인 위치를 찾는다.
    # pos_indices를 (B, 1)로 맞춰서 비교
    pos_indices = pos_indices.view(-1, 1)             # (B, 1)
    hit_matrix = (ranking == pos_indices)             # (B, C) True/False

    # hit_matrix[b]에서 True가 있는 위치 = 정답의 랭크 위치 (0-based)
    # 우리는 항상 정답을 후보에 넣었으므로, 각 row에 True가 정확히 1개 있다고 가정
    rank_positions = hit_matrix.float().argmax(dim=1)  # (B,)  0-based rank

    # 0-based → 1-based 랭크로 변환
    ranks = rank_positions + 1  # (B,)

    # 3) HR@K 계산: rank <= K 이면 hit
    hits = (ranks <= k).float()        # (B,)
    hr_k = hits.mean().item()

    # 4) NDCG@K 계산: hit인 경우에만 1 / log2(rank+1)
    ndcg = torch.zeros(B, device=device)
    # hit인 위치만 계산
    hit_mask = (ranks <= k)
    ndcg[hit_mask] = 1.0 / torch.log2(ranks[hit_mask].float() + 1.0)
    ndcg_k = ndcg.mean().item()

    return hr_k, ndcg_k

PAD_ID = 0
MAX_LEN = 50


# -----------------------------------------------------
# 1) JSON 로딩 + max_len 적용 패딩 함수
# -----------------------------------------------------
def load_sequences_from_json(path, max_len=MAX_LEN):
    with open(path, "r") as f:
        data = json.load(f)  # dict: user -> [item1, item2, ...]

    sequences = []
    for user, seq in data.items():
        seq = seq[-max_len:]  # 뒤에서 max_len개만 유지

        # 왼쪽 PAD 패딩
        padded = np.full((max_len,), PAD_ID, dtype=np.int64)
        padded[-len(seq):] = np.array(seq, dtype=np.int64)

        sequences.append(padded)

    sequences = np.stack(sequences, axis=0)  # (N, max_len)
    return sequences


# -----------------------------------------------------
# 2) Dataset 정의
# -----------------------------------------------------
class SASRecSequenceDataset(Dataset):
    def __init__(self, sequences):
        # sequences: (N, L) numpy array or tensor
        self.sequences = torch.tensor(sequences, dtype=torch.long)

    def __len__(self):
        return self.sequences.shape[0]

    def __getitem__(self, idx):
        return self.sequences[idx]  # (L,)


# -----------------------------------------------------
# 3) train / val / test split + dataloader 생성
# -----------------------------------------------------
def make_dataloaders(json_path,
                     max_len=MAX_LEN,
                     batch_size=256,
                     split_ratio=(0.8, 0.1, 0.1),
                     num_workers=0):
    seq = load_sequences_from_json(json_path, max_len=max_len)
    N = len(seq)

    # 유저 단위 셔플
    idx = np.arange(N)
    np.random.shuffle(idx)

    n_train = int(N * split_ratio[0])
    n_val   = int(N * split_ratio[1])

    train_idx = idx[:n_train]
    val_idx   = idx[n_train:n_train+n_val]
    test_idx  = idx[n_train+n_val:]

    train_ds = SASRecSequenceDataset(seq[train_idx])
    val_ds   = SASRecSequenceDataset(seq[val_idx])
    test_ds  = SASRecSequenceDataset(seq[test_idx])

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,
                              num_workers=num_workers, pin_memory=True)
    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,
                              num_workers=num_workers, pin_memory=True)
    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,
                              num_workers=num_workers, pin_memory=True)

    # num_items 추출 (PAD=0 제외)
    num_items = int(seq.max()) + 1

    return train_loader, val_loader, test_loader, num_items

def fixed_positional_encoding(seq_len, d_model):

    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1)
    # exp(-(2i/d_model) * log(10000)) -> 10000^-(2i/d_model)
    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)

    return pe

def make_attention_mask(seq, pad_id=0):
    """
    seq: (B, L)  - item id 시퀀스 (PAD_ID = pad_id)
    return: (B, L, L)  - 1: 볼 수 있음, 0: 가려야 함
    """
    B, L = seq.shape
    device = seq.device

    # 1) padding mask: key 위치 기준 (B, 1, L)
    # pad가 아닌 곳 = 1, pad = 0
    non_pad = (seq != pad_id).int().unsqueeze(1)      # (B, 1, L)

    # 2) causal mask: (1, L, L), j <= i 이면 1
    causal = torch.tril(torch.ones(L, L, device=device, dtype=torch.int))  # (L, L)
    causal = causal.unsqueeze(0)                                           # (1, L, L)

    # 3) 브로드캐스트 후 곱하기 → (B, L, L)
    attn_mask = non_pad * causal   # 1 or 0

    return attn_mask

# 차이점 hybrid positional encoding을 적용, pre LayerNorm 사용
"""
concat → Linear로 다시 d_model로 압축
pos_cat = concat(pos_learned, pos_sin)
FFN의 차원을 늘린 후 다시 줄임
pos = Linear(2d → d)
"""

# 모델의 차원은 홀수임을 가정
class SASRec(nn.Module):
  def __init__(self,
        num_items,
        max_len,
        d_model=42,
        n_layers=2,
        d_ff = None,
        dropout=0.2,
        share_embd = True,
        fixed_pos_embd = 0,  # 0: 안함 / 1: fixed embd를 더함 / 2: fixed embd를 concat
        pad_id=0
        ):
    super().__init__()
    self.num_items = num_items
    self.max_len = max_len
    self.d_model = d_model
    self.pad_id = pad_id

    # x = item_emb(seq) + pos_emb(pos_ids)
    self.embedding = nn.Embedding(num_items, d_model, padding_idx=pad_id)
    """
    아이템 인덱스 주의하기
    """
    self.positional_embedding = nn.Embedding(max_len, d_model)

    self.fixed_pos_embd = fixed_pos_embd
    if fixed_pos_embd == 2:
        self.hybrid_positional_encoding = nn.Linear(d_model * 2, d_model)

    self.share_embd = share_embd
    if share_embd is False:
      self.decoding = nn.Linear(d_model, num_items)

    self.encoders = nn.ModuleList(
      Encoder(d_model, d_ff, dropout)
      for _ in range(n_layers)
    )


    self.dropout = nn.Dropout(dropout)  # 임베딩용
    #self.layer_norm = nn.LayerNorm(d_model)

  def forward(self, x):
    attn_mask = make_attention_mask(x, self.pad_id)
    batch_size, seq_len = x.size()           # x: (B, L) = item id 시퀀스
    device = x.device

    # .unsqueeze(0) → (1, L) / (L,) → (1, L) 로 바꿔서 “배치 차원”을 하나 만들어 줌
    # .expand(batch_size, seq_len) → (B, L) / (1, L)을 (B, L)로 복제해서 쓰는 것처럼 만듬
    pos_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)

    x = self.embedding(x) + self.positional_embedding(pos_ids)
    if self.fixed_pos_embd == 1:
        pe = fixed_positional_encoding(seq_len, d_model=self.d_model).to(device)
        x = x + pe
    elif self.fixed_pos_embd == 2:
        pe = fixed_positional_encoding(seq_len, d_model=self.d_model).to(device)
        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # -1은 “기존 값 유지”라는 뜻
        x = torch.cat([x, pe], dim=-1)
        x = self.hybrid_positional_encoding(x)
    x = self.dropout(x)


    for layer in self.encoders:
      x = layer(x, mask=attn_mask)

    if self.share_embd:
      return x @ self.embedding.weight.T
    else:
      return self.decoding(x)



class SelfAttention(nn.Module):
  def __init__(self, d_model=42):
    super().__init__()
    self.d_model = d_model
    self.W_Q = nn.Linear(d_model, d_model)
    self.W_K = nn.Linear(d_model, d_model)
    self.W_V = nn.Linear(d_model, d_model)
    self.softmax = nn.Softmax(dim=-1)

  def forward(self, x, mask=None):
    Q = self.W_Q(x)
    K = self.W_K(x)
    V = self.W_V(x)

    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_model)
    if mask is not None:
      # out = x.masked_fill(mask, value)
      # mask가 True인 위치 → x의 값이 value로 바뀜
      # mask가 False인 위치 → 원래 값 유지
      scores = scores.masked_fill(mask == 0, -1e9)

    attn = self.softmax(scores)

    return attn @ V



class FFNLayer(nn.Module):
  def __init__(self, d_model=42, d_ff=None):
    super().__init__()
    if d_ff is None:
      d_ff = d_model * 4  # 관습적인 기본값

    self.FFN = nn.Sequential(
      nn.Linear(d_model, d_ff),
      nn.ReLU(),
      nn.Linear(d_ff, d_model)
    )

  def forward(self, x):
    return self.FFN(x)



class Encoder(nn.Module):
  def __init__(self, d_model=42, d_ff=None, dropout=0.2):
    super().__init__()
    self.SelfAtten = SelfAttention(d_model)
    self.FFN = FFNLayer(d_model, d_ff)
    self.layerNorm1 = nn.LayerNorm(d_model)
    self.layerNorm2 = nn.LayerNorm(d_model)
    self.dropout1 = nn.Dropout(dropout)
    self.dropout2 = nn.Dropout(dropout)

  def forward(self, x, mask=None): # Post-LN 대신 Pre-LN을 씀
    # --- Self-Attention sublayer ---
    residual = x
    x_norm = self.layerNorm1(x)                     # Pre-LN
    attn_out = self.SelfAtten(x_norm, mask)         # Attn(LN(x))
    x = residual + self.dropout1(attn_out)          # residual + dropout

    # --- FFN sublayer ---
    residual = x
    x_norm = self.layerNorm2(x)                     # Pre-LN
    ffn_out = self.FFN(x_norm)                      # FFN(LN(x))
    x = residual + self.dropout2(ffn_out)           # residual + dropout
    return x

def sasrec_ce_loss(logits, x, pad_id=0):
    """
    logits: (B, L, num_items)  - model output
    x:      (B, L)             - input sequence (item ids)
    """
    # 1) 타임시프트: t에서 t+1 예측
    logits = logits[:, :-1, :]      # (B, L-1, num_items)
    labels = x[:, 1:]               # (B, L-1)

    # 2) padding 위치는 loss에서 제외
    mask = (labels != pad_id)       # (B, L-1)  True = 유효

    if mask.sum() == 0:
        # 전부 PAD인 edge case 방지용
        return (logits * 0).sum()

    # 3) CrossEntropyLoss는 (N, C) vs (N,) 형태를 기대하므로 펼치기
    logits = logits.reshape(-1, logits.size(-1))  # (B*(L-1), num_items)
    labels = labels.reshape(-1)                   # (B*(L-1),)
    mask   = mask.reshape(-1)                     # (B*(L-1),)

    logits = logits[mask]     # 유효 위치만 남김
    labels = labels[mask]

    loss = F.cross_entropy(logits, labels)
    return loss

def train(model, datas, start_epoch, end_epoch, optimizer, device='cuda', print_loss = False):
    for epoch in range(start_epoch, end_epoch + 1):
        model.train()
        total_loss = 0
        total_steps = 0
        device = next(model.parameters()).device

        for batch in tqdm(datas):
            batch = batch.to(device)

            logits = model(batch)  # (B, L, num_items)
            loss = sasrec_ce_loss(logits, batch)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_steps += 1

        if print_loss:
            avg = total_loss / total_steps
            tqdm.write(f"Epoch {epoch} Loss: {avg:.4f}")

@torch.no_grad()
def test(model, data_loader, device='cuda', pad_id=0):
    model.eval()
    total_loss = 0.0
    total_steps = 0

    for batch in tqdm(data_loader):
        batch = batch.to(device)          # (B, L)
        logits = model(batch)             # (B, L, num_items)
        loss = sasrec_ce_loss(logits, batch, pad_id=pad_id)

        total_loss += loss.item()
        total_steps += 1

    if total_steps == 0:
        return 0.0

    return total_loss / total_steps

class SASRecPipeline():
    def __init__(self,
        max_len = MAX_LEN,
        batch_size = 128,
        split_ratio=(0.8, 0.1, 0.1),
        d_model=42,
        n_layers=2,
        d_ff = None,
        dropout=0.2,
        share_embd = True,
        fixed_pos_embd = 0,  # 0: 안함 / 1: fixed embd를 더함 / 2: fixed embd를 concat
        pad_id=0,
        lr=1e-3,
        device='cuda',
        json_path = None,
        num_workers=0,
        ):

        super().__init__()
        #self.num_items = num_items, self.max_len = max_len, self.d_model = d_model,
        self.train_data, self.val_data, self.test_data, self.num_items = make_dataloaders(
                      json_path,
                      max_len=max_len,
                      batch_size=batch_size,
                      split_ratio=split_ratio,
                      num_workers=num_workers)
        self.model = SASRec(self.num_items, max_len, d_model, n_layers, d_ff, dropout, share_embd, fixed_pos_embd, pad_id).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.epoch = 0
        self.device = device
        self.val_loss = {}
        self.val_ndcg = {}
        self.val_hit_rate = {}
        self.test_loss = -1
        self.test_ndcg = -1
        self.test_hit_rate = -1
        self.pad_id = pad_id

    def __call__(self, x):
        x = x.to(self.device)
        return self.model(x)

    def get_parameter(self):
        return self.model.parameters()

    def model_train(self, target_epoch, print_loss=False):
      # 이미 target_epoch까지 배웠으면 더 할 거 없음
      if target_epoch <= self.epoch:
          print(f'epoch value(={target_epoch}) is too small! (current={self.epoch})')
          return

      start = self.epoch + 1           # ✅ 기존 epoch 다음부터
      end   = target_epoch             # ✅ target_epoch까지

      train(
          model=self.model,
          datas=self.train_data,
          start_epoch=start,
          end_epoch=end,
          optimizer=self.optimizer,
          device=self.device,
          print_loss=print_loss,
      )

      self.epoch = target_epoch


    def model_validate(self, k=10):
        val_loss = test(model=self.model, data_loader=self.val_data, device=self.device, pad_id=self.pad_id)
        self.val_loss[self.epoch] = val_loss
        hit_rate, ndcg = self.evaluate_sasrec_with_negatives(k=k, validate=True)
        self.val_hit_rate[self.epoch] = [k, hit_rate]
        self.val_ndcg[self.epoch] = [k, ndcg]
        print(f"Validate loss: {val_loss:.4f} / Hit@{k}: {hit_rate:.4f} / NDCG@{k}: {ndcg:.4f}")

    def model_test(self, k=10):
        self.test_loss = test(model=self.model, data_loader=self.test_data, device=self.device, pad_id=self.pad_id)
        hit_rate, ndcg = self.evaluate_sasrec_with_negatives(k=k, validate=False)
        self.test_hit_rate = [k, hit_rate]
        self.test_ndcg = [k, ndcg]
        print(f"Validate loss: {self.test_loss:.4f} / Hit@{k}: {hit_rate:.4f} / NDCG@{k}: {ndcg:.4f}")

    def save_model(self, path=None):
        checkpoint = {
            "model_state": self.model.state_dict(),
            "optimizer_state": self.optimizer.state_dict(),
            "epoch": self.epoch,
            "val_loss": self.val_loss,
            "val_hit_rate": self.val_hit_rate,
            "val_ndcg": self.val_ndcg,
            "test_loss": self.test_loss,
            "test_hit_rate": self.test_hit_rate,
            "test_ndcg": self.test_ndcg,
            "config": {
                "num_items": self.model.num_items,
                "max_len": self.model.max_len,
                "d_model": self.model.d_model,
                "n_layers": len(self.model.encoders),
                "pad_id": self.model.pad_id,
            }
        }

        if path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = f"checkpoints/sasrec_plus_{timestamp}.pt"

        os.makedirs(os.path.dirname(path), exist_ok=True)

        torch.save(checkpoint, path)
        print(f"[INFO] Model saved to {path}")

    def load_model(self, path=None):
        if path is None:
            print('parh is needed!!!')
            return
        checkpoint = torch.load(path, map_location=self.device)

        # 모델 상태 로드
        self.model.load_state_dict(checkpoint["model_state"])

        # 옵티마이저 상태 로드
        self.optimizer.load_state_dict(checkpoint["optimizer_state"])

        # epoch / loss 히스토리 복구
        self.epoch = checkpoint.get("epoch", 0)
        self.val_loss = checkpoint.get("val_loss", {})
        self.test_loss = checkpoint.get("test_loss", -1)

        print(f"[INFO] Model loaded from {path}, epoch={self.epoch}")

    @torch.no_grad()
    def evaluate_sasrec_with_negatives(
        self,
        k: int = 10,
        num_negatives: int = 100,
        pad_id: int = 0,
        validate = True
    ):
        self.model.eval()
        device = next(self.model.parameters()).device

        if validate:
            data_loader = self.val_data
        else:
            data_loader = self.test_data

        all_hr = []
        all_ndcg = []

        for batch in data_loader:
            # 1) batch 형식 처리
            if isinstance(batch, (list, tuple)):
                # 혹시 나중에 (seq, pos)로 바꿔도 대응 가능하도록
                if len(batch) == 2:
                    seq, pos = batch
                else:
                    seq = batch[0]
                    pos = None
            else:
                seq = batch
                pos = None

            seq = seq.to(device)          # (B, L)
            B, L = seq.shape

            # 2) pos가 따로 없으면, 마지막 아이템을 정답으로 사용
            if pos is None:
                pos = seq[:, -1].clone()  # (B,)

            # 3) 모델 입력용 history를 만들고 싶다면:
            #    마지막 아이템은 가리고, 그 앞까지로 history 구성
            #    (논문식 leave-one-out을 흉내내는 느낌)
            seq_input = seq.clone()
            seq_input[:, -1] = pad_id     # target 위치를 PAD로 비워둔다

            # 4) 모델 forward → (B, L, num_items)
            logits = self.model(seq_input)

            # 5) "마지막-1 위치"에서의 hidden으로 마지막 아이템을 예측했으니까
            #    logits[:, -2, :]를 써서 scoring 하는 게 더 자연스럽다.
            #    (학습 때도 t에서 t+1을 예측하니까)
            last_step_logits = logits[:, -2, :]    # (B, num_items)

            # 6) 각 유저별로 candidate (1 pos + num_negatives neg) 구성
            candidate_ids = torch.zeros(B, 1 + num_negatives,
                                        dtype=torch.long, device=device)
            pos_indices = torch.zeros(B, dtype=torch.long, device=device)  # 항상 0

            for b in range(B):
                # 유저가 상호작용했던 item들을 제외 (seq 전체 + 정답)
                user_hist = set(seq[b].tolist())
                user_hist.discard(pad_id)              # PAD 제거
                user_hist.add(pos[b].item())           # 정답도 제외 대상에 포함

                negatives = []
                while len(negatives) < num_negatives:
                    # 아이템 id 범위: 1 ~ self.num_items-1 (pad_id=0 가정)
                    neg_id = random.randint(1, self.num_items - 1)
                    if neg_id not in user_hist and neg_id not in negatives:
                        negatives.append(neg_id)

                candidates = [pos[b].item()] + negatives
                candidate_ids[b] = torch.tensor(candidates,
                                                device=device,
                                                dtype=torch.long)

            # 7) last_step_logits에서 candidate들에 대한 점수만 뽑기
            scores = torch.gather(last_step_logits, dim=1, index=candidate_ids)

            # 8) HR@K, NDCG@K 계산
            hr_k, ndcg_k = hr_ndcg_at_k(scores, pos_indices, k=k)
            all_hr.append(hr_k)
            all_ndcg.append(ndcg_k)

        if len(all_hr) == 0:
            return 0.0, 0.0

        mean_hr = sum(all_hr) / len(all_hr)
        mean_ndcg = sum(all_ndcg) / len(all_ndcg)

        return mean_hr, mean_ndcg

"""

:()

from google.colab import drive

drive.mount('/content/drive')

#############################

devide = 'cuda' if torch.cuda.is_available() else 'cpu'

pipe = SASRecPipeline(max_len=20, device=devide, json_path="/content/drive/MyDrive/Colab_Notebooks/Recommendation_system/team_project/shorts_datasets/MicroLens-100k_user_sequences.json")

"""